{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Introduction","text":"<p>This is my end-to-end documentation to build, rebuild, and maintain my homelab.</p> <p>Some script snippets are very specific to my homelab, but most of the documentation should be generic enough to be useful for others.</p>"},{"location":"authentik/authentik/","title":"Authentik","text":"<pre><code># Create a namespace and generate secrets\nkubectl create namespace authentik\n# could not get it to work with these secrets, so using the authentik-credentials.yaml file instead\n# kubectl create secret generic authentik-secret-key --namespace authentik --from-literal=secret-key=$(openssl rand -base64 48)\n# kubectl create secret generic authentik-postgresql-password --namespace authentik --from-literal=secret-key=$(openssl rand -base64 48)\n# create certificate for authentik\nkubectl apply -f authentik-certificate.yaml\n# kubectl apply -n authentik -f authentik-secrets.yaml\nhelm repo add authentik https://charts.goauthentik.io\nhelm repo update\n# Install authentik - authentik-credentials.yaml is found in password vault\nhelm upgrade --install authentik authentik/authentik --namespace=authentik -f authentik-values.yaml,authentik-credentials.yaml\n</code></pre> <p>First check the database is healthy:</p> <pre><code>kubectl describe pods -n authentik -l app.kubernetes.io/name=postgresql\n</code></pre> <p>Next, check the authentik server and worker pods:</p> <pre><code>kubectl describe pods -n authentik -l app.kubernetes.io/name=authentik,app.kubernetes.io/component=server\nkubectl describe pods -n authentik -l app.kubernetes.io/name=authentik,app.kubernetes.io/component=worker\n</code></pre> <p>And finally, check the Redis pod:</p> <pre><code>kubectl describe pods -n authentik -l app.kubernetes.io/name=redis\n</code></pre> <p>Also check the certificate status, tls secret, and ingress configuration:</p> <pre><code>kubectl describe certificates authentik-tls -n authentik\nkubectl get secret authentik-tls -n authentik -o yaml\nkubectl get ingress -n authentik -o yaml\n</code></pre> <p>Can also try and restart the authentik server:</p> <pre><code>kubectl rollout restart deployment authentik-server -n authentik\n</code></pre> <p>If it looks good and pods are running we can check the service:</p> <pre><code>curl -kv https://authentik.local.spaelling.xyz/\n</code></pre> <p>If it says:</p> <pre><code>* Server certificate:\n*  subject: CN=TRAEFIK DEFAULT CERT\n</code></pre> <p>then the certificate is not in use. Likely an issue with Traefik. Check the Traefik dashboard if TLS is configured.</p>"},{"location":"authentik/authentik/#first-login","title":"First Login","text":"<p>Go to https://authentik.local.spaelling.xyz/if/flow/initial-setup/</p> <p>Create a new user and add it to the admins group.</p> <p>Go to <code>settings</code> and under <code>MFA devices</code> register a passkey. Deactivate the <code>akadmin</code> user.</p>"},{"location":"authentik/authentik/#logs","title":"Logs","text":"<pre><code>kubectl logs -n authentik -l app.kubernetes.io/name=authentik,app.kubernetes.io/component=server --tail=-1 &gt; authentik-server.log\nkubectl logs -n authentik -l app.kubernetes.io/name=postgresql --tail=-1 &gt; authentik-postgresql.log\n</code></pre>"},{"location":"authentik/authentik/#uninstall","title":"Uninstall","text":"<p>Persistent Volume Claims (PVCs) are not installed by <code>helm uninstall</code>, so you may need to delete them manually if you want to clean up completely:</p> <pre><code>helm uninstall authentik --namespace=authentik\nkubectl get pvc -n authentik -o jsonpath=\"{.items[*].metadata.name}\" | tr ' ' '\\n' | xargs -I {} kubectl delete pvc {} -n authentik\n</code></pre> <pre><code>kubectl delete secret authentik-secret-key --namespace authentik\nkubectl delete secret authentik-postgresql-password --namespace authentik\n</code></pre>"},{"location":"authentik/authentik/#certificates","title":"Certificates","text":"<p>We need both a certificate for the authentik service, but also for the outpost.</p>"},{"location":"dns/pihole/pihole/","title":"Pihole","text":"<p>Apply the ConfigMap for the Traefik Cloud Provider (<code>kubevip-configmap.yaml</code> is in the <code>k3s/kubevip</code> directory):</p> <pre><code>kubectl apply -f kubevip-configmap.yaml\nkubectl apply -f pihole.yaml\nkubectl create secret -n pihole generic pihole-webpassword --from-literal=\"password=$(openssl rand -base64 64)\"\n</code></pre> <p>It has a dedicated IP for the Pihole namespace, <code>192.168.1.21</code>.</p> <p>When pods are running test <code>tcping -f 4 -t 5 192.168.1.21 53</code> and <code>nslookup google.com 192.168.1.21</code></p>"},{"location":"dns/pihole/pihole/#todo","title":"TODO","text":"<ul> <li>Expose API</li> <li>Deploy Nebula sync</li> <li>sync with main pihole instance</li> <li>Access to web ui via authentik</li> </ul>"},{"location":"home_assistant/home_assistant/","title":"Home Assistant","text":"<pre><code>kubectl create namespace home-assistant\nkubectl apply -f home_assistant.yaml\n</code></pre> <p>If <code>configmap</code> is updated then update the deployment</p> <pre><code>kubectl rollout restart deployment home-assistant -n home-assistant\nkubectl rollout status  deployment home-assistant -n home-assistant\n</code></pre> <p>If this hangs we can force it by scaling down to 0 and back up again</p> <pre><code>kubectl -n home-assistant scale deploy/home-assistant --replicas=0\nkubectl -n home-assistant get pods\n\nkubectl -n home-assistant delete rs -l app=home-assistant\n\nkubectl -n home-assistant scale deploy/home-assistant --replicas=1\nkubectl -n home-assistant rollout status deploy/home-assistant\n</code></pre> <p>TODO. implement with kustomize</p> <pre><code>spec:\n  template:\n    metadata:\n      annotations:\n        # Update this value when the ConfigMap changes.\n        configmap.home-assistant.checksum: \"{{ .Values or kustomize var with sha256 of the ConfigMap }}\"\n</code></pre>"},{"location":"home_assistant/home_assistant/#troubleshooting","title":"Troubleshooting","text":"<p>Check the status of the certificate in the <code>traefik</code> namespace:</p> <pre><code>kubectl get certificates -n home-assistant --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe certificates {} -n home-assistant\n\nkubectl get certificaterequests -n home-assistant --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe certificaterequests {} -n home-assistant\n\nkubectl get order -n home-assistant --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe order {} -n home-assistant\n\nkubectl get challenges -n home-assistant --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe challenges {} -n home-assistant\n</code></pre> <pre><code>kubectl exec -n home-assistant -it deploy/home-assistant -- sh\n</code></pre>"},{"location":"home_assistant/home_assistant/#matter-server","title":"Matter Server","text":"<pre><code>kubectl apply -f matter_server.yaml\n</code></pre> <pre><code>kubectl logs -f deployment/matter-server -n home-assistant\n</code></pre> <p>Look for these entries in the log:</p> <pre><code>INFO [matter_server.server.server] Starting the Matter Server...\nINFO [matter_server.server.helpers.paa_certificates] Fetching the latest PAA root certificates...\nINFO [matter_server.server.server] Matter Server successfully initialized.\n</code></pre> <p>Just like with Home Assistant updates may get stuck on old replicas.</p> <pre><code>kubectl -n home-assistant scale deploy/matter-server --replicas=0\nkubectl -n home-assistant get pods\n\nkubectl -n home-assistant delete rs -l app=matter-server\n\nkubectl -n home-assistant scale deploy/matter-server --replicas=1\nkubectl -n home-assistant rollout status deploy/matter-server\n</code></pre>"},{"location":"home_assistant/home_assistant/#testing-matter-server-connectivity","title":"Testing Matter Server Connectivity","text":"<pre><code>kubectl get pod -n home-assistant -l app=matter-server -o jsonpath='{.items[0].status.hostIP}'\ntcping -f 4 -t 5 192.168.1.12 5580\n</code></pre>"},{"location":"k3s/housekeeping/","title":"Housekeeping","text":"<p>Below are some basic housekeeping tasks to perform before installing K3s.</p>"},{"location":"k3s/housekeeping/#passwords","title":"Passwords","text":"<p>Make sure to change the password for the logged in user and root user</p> <pre><code>passwd\npasswd root\n</code></pre>"},{"location":"k3s/housekeeping/#update","title":"Update","text":"<p>Keep the system up to date by running the following commands:</p> <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y\n</code></pre>"},{"location":"k3s/k3sup/","title":"k3sup","text":"<p>k3sup is a light-weight utility to get from zero to KUBECONFIG with k3s on any local or remote VM.</p> <p>Install k3sup using Homebrew:</p> <pre><code>brew install k3sup\n</code></pre> <p>NOTE:</p> <p>Most commands in the following assume superuser privileges, so you may want to run them with <code>sudo</code> or switch to the root user (<code>su</code>)</p>"},{"location":"k3s/k3sup/#install-k3s-master-node","title":"Install k3s Master Node","text":"<p>Note: If there is an existing kubeconfig file from a previous installation, you may want to remove it first <code>rm -f ~/.kube/config</code>.</p> <pre><code>k3sup install \\\n    --user pi \\\n    --sudo true \\\n    --ip 192.168.1.11 \\\n    --cluster \\\n    --k3s-channel stable \\\n    --no-extras \\\n    --local-path ~/.kube/config \\\n    --merge \\\n    --tls-san 192.168.1.10 \\\n    --tls-san k3s.local.spaelling.xyz\n</code></pre> <p>validate that k3s is running</p> <pre><code>export KUBECONFIG=~/.kube/config\nkubectl config use-context default\nkubectl get node -o wide\n</code></pre> <p>and check the status of the k3s service</p> <pre><code>systemctl status k3s.service\ncat /etc/systemd/system/k3s.service\n</code></pre> <p>Look specifically for <code>--tls-san k3s.local.spaelling.xyz --tls-san 192.168.1.10 --disable servicelb --disable traefik</code>. If any of these parameters are missing the installation did not go as planned.</p> <p>If missing add to the <code>k3s.service</code> file and restart the service:</p> <pre><code>systemctl daemon-reload\nsystemctl restart k3s.service\n</code></pre>"},{"location":"k3s/k3sup/#join-a-new-server-to-the-cluster","title":"Join a new server to the cluster","text":"<pre><code>k3sup join \\\n    --user pi \\\n    --ip 192.168.1.12 \\\n    --sudo true \\\n    --server \\\n    --server-ip 192.168.1.11 \\\n    --server-user pi \\\n    --k3s-channel stable \\\n    --no-extras\n</code></pre>"},{"location":"k3s/k3sup/#uninstall-k3s","title":"Uninstall k3s","text":"<p>Simply run the uninstall script on each node:</p> <pre><code>sudo /usr/local/bin/k3s-uninstall.sh\n</code></pre>"},{"location":"k3s/kubevip/","title":"Kubevip","text":"<p>Following instructions on k3s. Full script towards the end.</p> <p>Create Manifests Folder</p> <pre><code>mkdir -p /var/lib/rancher/k3s/server/manifests/\n</code></pre> <p>K3s has an optional manifests directory that will be searched to auto-deploy any manifests found within.</p> <p>Upload kube-vip RBAC Manifest</p> <pre><code>curl https://kube-vip.io/manifests/rbac.yaml &gt; /var/lib/rancher/k3s/server/manifests/kube-vip-rbac.yaml\n</code></pre>"},{"location":"k3s/kubevip/#generate-a-kube-vip-daemonset-manifest","title":"Generate a kube-vip DaemonSet Manifest","text":"<pre><code>export VIP=192.168.1.10\nexport INTERFACE=eth0\napt install -y jq curl\nKVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r \".[0].name\")\nalias kube-vip=\"ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION; ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip\"\n</code></pre> <p>Generate manifest and save to <code>kube-vip.yaml</code>:</p> <pre><code>kube-vip manifest daemonset \\\n    --interface $INTERFACE \\\n    --address $VIP \\\n    --inCluster \\\n    --taint \\\n    --controlplane \\\n    --services \\\n    --arp \\\n    --leaderElection | tee /var/lib/rancher/k3s/server/manifests/kube-vip.yaml\n</code></pre> <pre><code>mkdir -p /var/lib/rancher/k3s/server/manifests/\ncurl https://kube-vip.io/manifests/rbac.yaml &gt; /var/lib/rancher/k3s/server/manifests/kube-vip-rbac.yaml\nexport VIP=192.168.1.10\nexport INTERFACE=eth0\napt install -y jq curl\nKVVERSION=$(curl -sL https://api.github.com/repos/kube-vip/kube-vip/releases | jq -r \".[0].name\")\nalias kube-vip=\"ctr image pull ghcr.io/kube-vip/kube-vip:$KVVERSION; ctr run --rm --net-host ghcr.io/kube-vip/kube-vip:$KVVERSION vip /kube-vip\"\nkube-vip manifest daemonset \\\n    --interface $INTERFACE \\\n    --address $VIP \\\n    --inCluster \\\n    --taint \\\n    --controlplane \\\n    --services \\\n    --arp \\\n    --leaderElection | tee /var/lib/rancher/k3s/server/manifests/kube-vip.yaml\n</code></pre> <p>This is now automatically deployed by k3s, and you can check the status of the kube-vip pods:</p> <pre><code>kubectl get pods -n kube-system\n</code></pre> <p>Add a DNS entry for the VIP in the DNS server, in this case A-record <code>k3s.local.spaelling.xyz=192.168.1.10</code></p> <p>edit kubeconfig <code>sudo nano ~/.kube/config</code> and change the server address to <code>k3s.local.spaelling.xyz</code>.</p>"},{"location":"k3s/kubevip/#install-the-kube-vip-cloud-provider","title":"Install the kube-vip Cloud Provider","text":"<pre><code>kubectl apply -f https://raw.githubusercontent.com/kube-vip/kube-vip-cloud-provider/main/manifest/kube-vip-cloud-controller.yaml\n</code></pre> <p>We will create kube-vip Cloud Provider ConfigMap later when Traefik is up and running as we want it handle all ingress traffic.</p>"},{"location":"k3s/longhorn/","title":"Longhorn","text":"<p>Installation Requirements</p> <p>Checking Prerequisites Using Longhorn Command Line Tool</p> <pre><code>sudo mkdir -p ~/.kube\nsudo cp /etc/rancher/k3s/k3s.yaml ~/.kube/config\n</code></pre> <pre><code>curl -sSfL -o longhornctl https://github.com/longhorn/cli/releases/download/v1.9.0/longhornctl-linux-arm64\n\nsudo chmod +x longhornctl\nsudo KUBECONFIG=/etc/rancher/k3s/k3s.yaml ./longhornctl check preflight\n</code></pre> <p>We can attempt to have Longhorn install prerequisites. This will install prerequisites on all nodes in the cluster.</p> <pre><code>sudo ./longhornctl --kube-config ~/.kube/config --image longhornio/longhorn-cli:v1.9.0 install preflight\n</code></pre>"},{"location":"k3s/longhorn/#helm-installation","title":"Helm Installation","text":"<p>Install with Helm</p> <pre><code>brew install helm\n</code></pre> <p>And then install Longhorn using Helm.</p> <pre><code>helm repo add longhorn https://charts.longhorn.io\nhelm repo update\nhelm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.9.0\nkubectl -n longhorn-system get pod\n</code></pre>"},{"location":"k3s/longhorn/#proxy","title":"Proxy","text":"<p>When Traefik is up and running we can create an ingress and certificate for Longhorn.</p> <p>The existing frontend service is already behind a load balancer, so we delete that and create our own.</p> <pre><code>kubectl delete service longhorn-frontend -n longhorn-system\nkubectl apply -f longhorn-frontend.yaml\nkubectl apply -f certificate.yaml\nkubectl apply -f ingress.yaml\n</code></pre> <p>Wait for the certificate to be ready.</p> <pre><code>kubectl describe certificate longhorn-web-ui-tls -n longhorn-system\n</code></pre>"},{"location":"k3s/sources/","title":"Sources","text":"<ul> <li>k3sup</li> <li>kube-vip</li> </ul>"},{"location":"k3s/ssh/","title":"SSH","text":"<p>ssh key must exist, ex. <code>~/.ssh/id_rsa.pub</code>. Use <code>ssh-keygen</code> to create a private and public key pair.</p> <pre><code>ssh-keygen -t rsa -b 4096 -C \"comment\"\n</code></pre> <p>Copy the SSH key to the target server</p> <pre><code>ssh-copy-id -i ~/.ssh/id_rsa.pub pi@192.168.1.11\n</code></pre> <p>Test the SSH connection</p> <pre><code>ssh -i ~/.ssh/id_rsa pi@192.168.1.11\n</code></pre>"},{"location":"k3s/prerequisites/rpi.prereqs/","title":"Raspberry Pi","text":"<p>To run K3s on a Raspberry Pi, we need to ensure that the system is properly configured. Below are the steps to prepare Raspberry Pi for K3s installation.</p> <p>On the Raspberry Pi modify <code>/boot/firmware/cmdline.txt</code> to include <code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code></p> <pre><code>nano /boot/firmware/cmdline.txt\n</code></pre> <p>Paste in <code>cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory</code> (make sure it is all on one line)</p> <p>After a rebot verify with</p> <pre><code># Show the *effective* kernel cmdline after boot\ncat /proc/cmdline\n</code></pre> <p>edit <code>/etc/dphys-swapfile</code> and set <code>CONF_SWAPSIZE=0</code></p> <pre><code>nano /etc/dphys-swapfile\n</code></pre> <p>then run the following commands to disable and reconfigure swap</p> <pre><code>apt update\napt install dphys-swapfile -y\napt remove systemd-zram-generator -y\ndphys-swapfile swapoff\ndphys-swapfile setup\n</code></pre>"},{"location":"k3s/prerequisites/rpi.prereqs/#static-ip-address","title":"Static IP address","text":"<p>A static IP address is recommended for the Raspberry Pi to ensure consistent connectivity.</p> <pre><code>nano /etc/hosts\n</code></pre> <p>and change 127.0.0.1 to desired static IP address</p> <p>This should return the static IP address</p> <pre><code>hostname --ip-address\n</code></pre> <p>Next use <code>nmcli</code> to set a static IP address.</p> <pre><code>nmcli connection add type ethernet ifname eth0 con-name static-eth0 ipv4.addresses 192.168.1.11/24 ipv4.gateway 192.168.1.1 ipv4.dns 192.168.1.60 ipv4.method manual\nnmcli connection up static-eth0\n</code></pre> <p>use <code>ip a</code> to verify the static IP address is set correctly</p>"},{"location":"lgtm/grafana/grafana/","title":"Grafana","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nkubectl apply -f certificate.yaml\nkubectl apply -f grafana-oauth-secret.yaml\nhelm install grafana grafana/grafana --namespace lgtm -f grafana-values.yaml\n</code></pre> <p>All Grafana helm chart values can be found in Grafana Helm Chart.</p> <p>Get your 'admin' user password by running:</p> <pre><code>kubectl get secret --namespace lgtm grafana -o jsonpath=\"{.data.admin-password}\" | base64 --decode ; echo\n</code></pre> <p>We will need it to make other users admin.</p> <p>And test the installation by running:</p> <pre><code>curl -kv https://grafana.local.spaelling.xyz/\n</code></pre>"},{"location":"lgtm/grafana/grafana/#upgrade","title":"Upgrade","text":"<pre><code>helm upgrade grafana grafana/grafana --namespace lgtm -f grafana-values.yaml\n</code></pre> <p>Sometimes the previous pods are not delete, and the new gets stuck in an error state. Just delete some pods and it should resolve itself.</p>"},{"location":"lgtm/grafana/grafana/#authentik","title":"Authentik","text":"<p>Follow the instructions in the authentik documentation to set up Grafana. The helm values are already configured.</p>"},{"location":"lgtm/loki/loki/","title":"Loki","text":"<pre><code>helm repo add grafana https://grafana.github.io/helm-charts\nhelm repo update\nhelm install loki grafana/loki -f loki-values.yaml -n lgtm\nkubectl get pods -n lgtm\n</code></pre>"},{"location":"lgtm/loki/loki/#upgrade-loki","title":"Upgrade Loki","text":"<pre><code>helm upgrade loki grafana/loki --values loki-values.yaml -n lgtm\n</code></pre> <p>You can send logs from inside the cluster using the cluster DNS:</p> <p>http://loki-gateway.lgtm.svc.cluster.local/loki/api/v1/push</p> <p>If Grafana operates within the cluster, you'll set up a new Loki datasource by utilizing the following URL:</p> <p>http://loki-gateway.lgtm.svc.cluster.local/</p>"},{"location":"lgtm/loki/loki/#kubernetes-monitoring","title":"Kubernetes Monitoring","text":"<p>https://grafana.com/docs/loki/latest/send-data/k8s-monitoring-helm/</p> <pre><code>helm install k3smon grafana/k8s-monitoring --values k3s-monitoring-values.yml -n lgtm\n</code></pre> <p>https://github.com/grafana/alloy/blob/main/operations/helm/charts/alloy/values.yaml</p> <p>Check logs to validate that it is sending logs to Loki</p> <pre><code>helm upgrade k3smon grafana/k8s-monitoring --values k3s-monitoring-values.yml -n lgtm\n</code></pre>"},{"location":"lgtm/prometheus/prometheus/","title":"prometheus","text":"<p>prometheus-community</p> <pre><code>helm repo add prometheus-community https://prometheus-community.github.io/helm-charts\nhelm repo update\nhelm install prometheus prometheus-community/kube-prometheus-stack --values prometheus-values.yaml -n lgtm\n\nkubectl --namespace lgtm get pods -l \"release=prometheus\"\n</code></pre> <p>Server url for the Grafana connection is: <code>http://prometheus-kube-prometheus-prometheus.lgtm.svc.cluster.local:9090</code></p>"},{"location":"servarr/servarr/","title":"Servarr","text":"<pre><code>kubectl create namespace servarr\n</code></pre>"},{"location":"servarr/jellyfin/jellyfin/","title":"Jellyfin","text":"<p>namespace <code>servarr</code> should already exist.</p> <pre><code>kubectl apply -k overlay\n</code></pre>"},{"location":"terminal/starship/starship/","title":"Starship","text":"<p>Starship is a cross-shell prompt that is fast, customizable, and works with many shells. It provides a rich set of features and can be easily configured to suit your needs.</p> <p>Follow installation instructions on the Starship website.</p> <pre><code>brew install starship\n</code></pre> <p>Download current starship.toml.</p>"},{"location":"terminal/warp/warp/","title":"Warp","text":"<p>I use Warp as my terminal. It is a modern terminal that is fast, customizable, and has a lot of features that make it easy to use.</p>"},{"location":"traefik/cert_manager/","title":"cert-manager","text":"<pre><code>brew install helm\n</code></pre> <p>Installing with Helm</p> <pre><code>helm repo add jetstack https://charts.jetstack.io --force-update\nhelm install \\\n  cert-manager jetstack/cert-manager \\\n  --namespace cert-manager \\\n  --create-namespace \\\n  --version v1.18.0 \\\n  --set crds.enabled=true\nkubectl -n cert-manager get pod\n</code></pre>"},{"location":"traefik/cert_manager/#issuer-configuration","title":"Issuer Configuration","text":"<p>Create a secret in Kubernetes containing the Cloudflare API token.</p> <p>Go to Cloudflare dashboard - click Create Token and use the <code>Edit zone DNS</code> template. Pick the specific zone resource. Restrict the client IP.</p> <p>For details on yaml configuration, see:</p> <p>Cloudflare API tokens</p> <p>Creating a ACME ClusterIssuer</p> <pre><code>cd ~/git/Privat/homelab-pages/docs/traefik\nkubectl apply -n cert-manager -f cloudflare_api_token.yaml\nkubectl apply -f acme_clusterissuer.yaml\n</code></pre> <p>To ensure it does not use local DNS to verify acme challenges</p> <pre><code>kubectl edit deployment cert-manager -n cert-manager\n</code></pre> <p>and make the change using Vim:</p> <p>When Vim opens, you are in Normal Mode. You cannot type yet. Move your cursor to where you want to add the text. Press i on your keyboard. You should see -- INSERT -- at the bottom left. Now you can type or paste your changes. Press the Esc key. Type <code>:wq</code> \u2014 This stands for Write (save) and Quit. Press Enter.</p> <pre><code>containers:\n      - args:\n        - --v=2\n        - --cluster-resource-namespace=$(POD_NAMESPACE)\n        - --leader-election-namespace=kube-system\n        # --- ADD THESE TWO LINES ---\n        - --dns01-recursive-nameservers-only\n        - --dns01-recursive-nameservers=1.1.1.1:53,8.8.8.8:53\n        # ---------------------------\n        image: quay.io/jetstack/cert-manager-controller:v1.x.x\n</code></pre> <p>We can force a restart of the cert-manager pods to pick up the changes:</p> <pre><code>kubectl rollout restart deployment cert-manager -n cert-manager\n</code></pre>"},{"location":"traefik/cert_manager/#troubleshooting","title":"Troubleshooting","text":"<p>Check the status of the certificate in the <code>traefik</code> namespace:</p> <pre><code>kubectl get certificates -n traefik --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe certificates {} -n traefik\n\nkubectl get certificaterequests -n traefik --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe certificaterequests {} -n traefik\n\nkubectl get order -n traefik --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe order {} -n traefik\n\nkubectl get challenges -n traefik --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe challenges {} -n traefik\n</code></pre>"},{"location":"traefik/traefik/","title":"Traefik","text":"<p>Add Traefik Labs chart repository to Helm and install it with the Helm command line. All traefik helm chart values can be found here</p> <pre><code>helm repo add traefik https://traefik.github.io/charts\nhelm repo update\nkubectl create ns traefik\nhelm install --namespace=traefik --values=./traefik-values.yml traefik traefik/traefik\nkubectl get pods --namespace traefik\n</code></pre> <p>Get pods in the Traefik namespace and describe them to check their status:</p> <pre><code>kubectl get pods -n traefik --no-headers -o custom-columns=\":metadata.name\" | xargs -I {} kubectl describe pod {} -n traefik\n</code></pre> <p>We can upgrade a helm release with the following command (if ex. changing values in the <code>traefik-values.yml</code> file):</p> <pre><code>helm upgrade --namespace=traefik --values=./traefik-values.yml traefik traefik/traefik\n</code></pre> <p>Create a ConfigMap for the Traefik Cloud Provider (<code>kubevip-configmap.yaml</code> is in the <code>k3s/kubevip</code> directory):</p> <pre><code>kubectl apply -f kubevip-configmap.yaml\n</code></pre> <p>This allows kubevip to assign an external IP address to the Traefik service.</p> <p>Create certificate for Traefik dashboard:</p> <pre><code>kubectl apply -f dashboard-certificate.yaml\n</code></pre>"},{"location":"traefik/traefik/#troubleshooting","title":"Troubleshooting","text":"<p>Check the status of the Traefik IngressRoute and secrets:</p> <pre><code>kubectl describe ingressroute traefik-dashboard -n traefik\nkubectl describe secrets traefik-dashboard-tls -n traefik\n</code></pre> <p>Check the logs of the Traefik pods to see if there are any errors:</p> <pre><code>kubectl logs -n traefik -l app.kubernetes.io/name=traefik &gt; traefik.log\n</code></pre>"},{"location":"traefik/traefik/#authentik","title":"Authentik","text":"<p>Setup authentication forwarding to the authentik service.</p> <pre><code>kubectl apply -f authentik-middleware.yaml\n</code></pre>"}]}